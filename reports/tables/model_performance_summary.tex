\begin{table}[htbp]
  \centering
  \caption{Weighted test-set performance comparison across model classes and feature specifications.}
  \label{tab:model-performance}
  \small
  \begin{tabular}{llrrrrrr}
    \toprule
    \textbf{Model} & \textbf{Feature Set} & \textbf{ROC-AUC} & \textbf{PR-AUC} & \textbf{Brier Score} & \textbf{ECE} & \textbf{Balanced Acc.} & \textbf{Test N} \\
    \midrule
    LR (default) & Full (22 raw / 58 enc.) & 0.857 & 0.795 & 0.145 & 0.031 & 0.745 & 2155 \\
    LR (balanced) & Full (22 raw / 58 enc.) & 0.856 & 0.794 & 0.149 & 0.068 & 0.785 & 2155 \\
    GBM & Full (22 raw / 58 enc.) & 0.867 & 0.800 & 0.142 & 0.039 & 0.761 & 2155 \\
    GBM (calibrated) & Full (22 raw / 58 enc.) & 0.867 & 0.777 & 0.144 & 0.043 & 0.751 & 2155 \\
    \midrule
    Best Matched Raw (GBM) & Core Attitudes (9 raw) & 0.841 & 0.783 & 0.151 & 0.031 & 0.746 & 1743 \\
    Best Latent (LR) & Latent + Know. + Demo. & 0.835 & 0.769 & 0.156 & 0.056 & 0.736 & 1743 \\
    \bottomrule
  \end{tabular}
\end{table}
