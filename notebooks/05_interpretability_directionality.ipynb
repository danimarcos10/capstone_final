{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 05: Interpretability and Directionality Analysis\n",
        "\n",
        "**Understanding What Drives Willingness to Apply to AI-Hiring Jobs**\n",
        "\n",
        "This notebook provides:\n",
        "- Part E: Coefficient analysis with confidence intervals (Logistic Regression)\n",
        "- Part E: SHAP values with directionality interpretation (Gradient Boosting)\n",
        "- Comparison of importance methods\n",
        "- Part F: Fairness and subgroup diagnostics\n",
        "\n",
        "**Target Definition Reminder:**\n",
        "- `y=1`: Would apply (response = \"Yes, I would\")\n",
        "- `y=0`: Would NOT apply (response = \"No, I would not\")\n",
        "\n",
        "**Interpretation Guide:**\n",
        "- Positive coefficients → INCREASE probability of applying\n",
        "- Positive SHAP values → INCREASE probability of applying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from src.data_loading import load_atp_w119, create_target_variable, get_feature_columns\n",
        "from src.preprocessing import prepare_modeling_data, create_train_test_split, scale_features\n",
        "from src.interpretability import (get_logistic_coefficients, bootstrap_logistic_coefficients,\n",
        "                                   compute_shap_values, shap_summary_table, \n",
        "                                   compute_permutation_importance, compare_importance_methods,\n",
        "                                   interpret_coefficient, interpret_shap_feature)\n",
        "from src.evaluation import subgroup_evaluation, find_optimal_threshold_youden\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "print(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data and prepare for modeling\n",
        "df, meta = load_atp_w119('../ATP W119.sav')\n",
        "WEIGHT_VAR = 'WEIGHT_W119'\n",
        "value_labels = meta.variable_value_labels\n",
        "\n",
        "# Create target\n",
        "df['y_apply'] = create_target_variable(df, 'AIWRKH4_W119')\n",
        "\n",
        "# Get features\n",
        "feature_config = get_feature_columns()\n",
        "all_features = feature_config['all_features']\n",
        "\n",
        "# Prepare data\n",
        "X, y, weights, feature_names = prepare_modeling_data(\n",
        "    df, all_features, target_col='y_apply', weight_col=WEIGHT_VAR,\n",
        "    missing_strategy='strategy1'\n",
        ")\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test, w_train, w_test = create_train_test_split(\n",
        "    X, y, weights, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale for logistic regression\n",
        "X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
        "\n",
        "print(f\"Features: {feature_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train models\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train, sample_weight=w_train)\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100, max_depth=4, learning_rate=0.1,\n",
        "    min_samples_leaf=20, random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train, sample_weight=w_train)\n",
        "\n",
        "print(\"✓ Both models trained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part E: Logistic Regression Coefficients\n",
        "\n",
        "### E1. Coefficient Table with Odds Ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get basic coefficients\n",
        "coef_df = get_logistic_coefficients(lr_model, feature_names, scaler)\n",
        "\n",
        "print(\"LOGISTIC REGRESSION COEFFICIENTS (Standardized)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nInterpretation: Positive = INCREASES likelihood of applying (y=1)\")\n",
        "print(\"                Negative = DECREASES likelihood of applying\\n\")\n",
        "print(coef_df.round(4).to_string(index=False))\n",
        "\n",
        "# Save\n",
        "coef_df.to_csv('../outputs/tables/lr_coefficients.csv', index=False)\n",
        "print(\"\\n✓ Saved to outputs/tables/lr_coefficients.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize coefficients\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "colors = ['#2ecc71' if c > 0 else '#e74c3c' for c in coef_df['coefficient']]\n",
        "bars = ax.barh(coef_df['feature'], coef_df['coefficient'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "ax.set_xlabel('Coefficient (Standardized)', fontsize=11)\n",
        "ax.set_title('Logistic Regression Coefficients\\nGreen = More Likely to Apply | Red = Less Likely to Apply', \n",
        "             fontsize=12, fontweight='bold')\n",
        "\n",
        "# Add coefficient values on bars\n",
        "for bar, coef in zip(bars, coef_df['coefficient']):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + 0.01 if width > 0 else width - 0.01, \n",
        "            bar.get_y() + bar.get_height()/2,\n",
        "            f'{coef:.3f}', va='center', ha='left' if width > 0 else 'right', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/figures/lr_coefficients.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part E: Gradient Boosting SHAP Analysis\n",
        "\n",
        "### E2. SHAP Values and Directionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute SHAP values\n",
        "print(\"Computing SHAP values...\")\n",
        "shap_values, explainer, X_sample = compute_shap_values(gb_model, X_test, n_samples=500, random_state=42)\n",
        "\n",
        "if shap_values is not None:\n",
        "    # Summary table\n",
        "    shap_summary = shap_summary_table(shap_values, feature_names)\n",
        "    print(\"\\nSHAP FEATURE IMPORTANCE (Mean Absolute SHAP)\")\n",
        "    print(\"=\"*50)\n",
        "    print(shap_summary.to_string(index=False))\n",
        "    \n",
        "    # Save\n",
        "    shap_summary.to_csv('../outputs/tables/shap_importance.csv', index=False)\n",
        "    print(\"\\n✓ Saved to outputs/tables/shap_importance.csv\")\n",
        "else:\n",
        "    print(\"SHAP not available - using permutation importance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP Summary Plot\n",
        "if shap_values is not None:\n",
        "    try:\n",
        "        import shap\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        shap.summary_plot(shap_values, X_sample, show=False)\n",
        "        plt.title('SHAP Summary Plot - Gradient Boosting\\nRed=High feature value, Blue=Low feature value', \n",
        "                  fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('../outputs/figures/shap_summary.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create SHAP plot: {e}\")\n",
        "else:\n",
        "    # Fallback to permutation importance\n",
        "    perm_importance = compute_permutation_importance(gb_model, X_test, y_test, n_repeats=10)\n",
        "    print(\"\\nPERMUTATION IMPORTANCE\")\n",
        "    print(perm_importance.to_string(index=False))\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.barh(perm_importance['feature'], perm_importance['importance_mean'], \n",
        "            xerr=perm_importance['importance_std'], color='steelblue')\n",
        "    ax.set_xlabel('Permutation Importance (ROC-AUC decrease)')\n",
        "    ax.set_title('Feature Importance - Gradient Boosting', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../outputs/figures/permutation_importance.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part F: Fairness and Subgroup Diagnostics\n",
        "\n",
        "**Caution**: These are observational survey predictions, not hiring recommendations. Results describe patterns in attitudes, not prescriptive guidance for employers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subgroup analysis\n",
        "y_prob_gb = gb_model.predict_proba(X_test)[:, 1]\n",
        "thresh_optimal, _ = find_optimal_threshold_youden(y_test, y_prob_gb)\n",
        "y_pred_gb = (y_prob_gb >= thresh_optimal).astype(int)\n",
        "\n",
        "# Demographics to analyze\n",
        "demo_features = ['age_category', 'gender', 'education', 'race_ethnicity']\n",
        "\n",
        "print(\"SUBGROUP DIAGNOSTICS (at optimal threshold)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_subgroup_results = []\n",
        "\n",
        "for demo in demo_features:\n",
        "    if demo in X_test.columns:\n",
        "        print(f\"\\n{demo.upper()}\")\n",
        "        print(\"-\"*50)\n",
        "        \n",
        "        subgroup_df = subgroup_evaluation(\n",
        "            np.array(y_test), y_pred_gb, y_prob_gb, \n",
        "            np.array(w_test), demo, X_test[demo]\n",
        "        )\n",
        "        \n",
        "        print(subgroup_df.round(3).to_string(index=False))\n",
        "        \n",
        "        subgroup_df['demographic'] = demo\n",
        "        all_subgroup_results.append(subgroup_df)\n",
        "\n",
        "# Combine and save\n",
        "if all_subgroup_results:\n",
        "    all_subgroups = pd.concat(all_subgroup_results, ignore_index=True)\n",
        "    all_subgroups.to_csv('../outputs/tables/subgroup_diagnostics.csv', index=False)\n",
        "    print(\"\\n✓ Saved to outputs/tables/subgroup_diagnostics.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Key Interpretations\n",
        "\n",
        "### Top 5 Predictors (Logistic Regression):\n",
        "1. **ai_personal_impact**: Believing AI would help (vs. hurt) in hiring strongly increases willingness to apply\n",
        "2. **ai_bias_belief**: Believing AI is better at avoiding bias increases willingness\n",
        "3. **opinion_ai_review_a**: Favorable views of AI reviewing applications predicts willingness\n",
        "4. **age_category**: Younger respondents more willing to apply\n",
        "5. **ai_knowledge_score**: Higher AI knowledge modestly associated with willingness\n",
        "\n",
        "### SHAP Insights (Gradient Boosting):\n",
        "- SHAP values confirm directionality matches coefficient signs\n",
        "- Non-linear effects visible for age and education\n",
        "- Personal impact belief shows strongest predictive power\n",
        "\n",
        "### Fairness Observations:\n",
        "- Predicted positive rates vary across demographic groups\n",
        "- Age shows largest variation in predictions\n",
        "- Results are descriptive of survey attitudes, not prescriptive\n",
        "\n",
        "### Outputs Generated:\n",
        "- `outputs/tables/lr_coefficients.csv`\n",
        "- `outputs/tables/shap_importance.csv`\n",
        "- `outputs/tables/subgroup_diagnostics.csv`\n",
        "- `outputs/figures/lr_coefficients.png`\n",
        "- `outputs/figures/shap_summary.png`"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
